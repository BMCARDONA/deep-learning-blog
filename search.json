[
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Recent college graduate, and a deep learning practitioner!\nYou can find my personal website here."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bradley’s Deep Learning Blog",
    "section": "",
    "text": "Creating a Hot Dog Binary Classifer Using Fast.AI\n\n\n\n\n\n\n\ncode\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nBradley Cardona\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html",
    "href": "posts/hot-dog-classifier/index.html",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "",
    "text": "Having recently completed DeepLearning.AI’s wonderful Deep Learning Specialization, and having recently started fast.ai’s Practical Deep Learning for Coders, I thought I would try to implement a binary classifier to test whether an image of a food item belongs to the “hot dog” or “not hot dog” class, as seen on that Silicon Valley episode.\nTo create this project, I used the fast.ai library, Gradio, HuggingFace Spaces, this Kaggle dataset, and Google Colab. In this article, we will discuss the notebook I used to train my model, in addition to the steps I took to deploy it. Feel free to check out the deployed project here. :)"
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#training",
    "href": "posts/hot-dog-classifier/index.html#training",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Training",
    "text": "Training\nTo begin, we will import any necessary dependencies.\nfrom fastai.vision.all import *\nimport timm\nfrom google.colab import drive\nimport os\nSince we are using Google Colab to execute the notebook cells, we need to mount the Google Drive to the Colab notebook’s file system. (Mounting allows one to access and manipulate files stored in one’s Google Drive directly from within one’s Colab notebook.)\n# Mount Google Drive\ndrive.mount('/content/drive')\nHaving mounted my drive, let’s now specify the path to my dataset directory, which itself contains two additional subdirectories: hot-dog and not-hot-dog. The former contains photos of hot dogs, the latter photos of “not hot dogs.”\npath = '/content/drive/MyDrive/fast_ai_experiments/3_neural_net_foundations/hot_dog_not_hotdog/dataset/'\nEvery image in the hot-dog and not-hot-dog subdirectories has a pre-existing naming format of “number.jpg” (e.g., “1231.jpg”). For the sake of using a better naming format, let’s use the format of “hot-dog_index” (e.g., “hot-dog_12.jpg”) for each image in the hot-dog subdirectory, and “not-hot-dog_index” (e.g., “not-hot-dog_12.jpg”) for each image in the not-hot-dog subdirectory.\n# List of subdirectories\nsubdirectories = ['hot-dog', 'not-hot-dog']\n\n# Iterate through subdirectories\nfor subdir in subdirectories:\n    subdir_path = os.path.join(path, subdir)\n\n    # List all files in the subdirectory\n    file_list = os.listdir(subdir_path)\n\n    # Iterate through the files and rename them with a numbered sequence\n    for i, filename in enumerate(file_list, start=1):\n        if filename.endswith(\".jpg\"):\n            new_filename = f\"{subdir}_{i}.jpg\"\n            os.rename(os.path.join(subdir_path, filename), os.path.join(subdir_path, new_filename))\nNext, we will use the ImageDataLoaders.from_name_func() method. This is a fast.ai method used for creating “data loaders” for image classification tasks; it takes various arguments, which define how the data should be loaded and prepared.\nUsing this method, we will define the training/validation split as 80% for training and 20% for validation; we will label each image in the hot-dog subdirectory as “hot-dog” and each image in the not-hot-dog one as “not-hot-dog”; and we will re-size each image to be 224 x 224 in pixel size.\n# Creating ImageDataLoaders\ndls = ImageDataLoaders.from_name_func(\n    path,\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224),\n)\nLet’s now take a look at a batch containing 20 labeled images:\ndls.show_batch(max_n=20)\n\n\n\npng\n\n\nNice, it seems that each photo is labeled appropriately! Let’s now use the fast.ai library to harness the capabilities of transfer learning. We will create a learner object for image classification using the ResNet-34 architecture, train the model on our training set for 3 epochs, and then evaluate the model’s performance on our validation set using the “error rate” metric.\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.890783\n\n\n0.328621\n\n\n0.130653\n\n\n02:10\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.457683\n\n\n0.231882\n\n\n0.105528\n\n\n00:13\n\n\n\n\n1\n\n\n0.270772\n\n\n0.355318\n\n\n0.110553\n\n\n00:08\n\n\n\n\n2\n\n\n0.187048\n\n\n0.347728\n\n\n0.105528\n\n\n00:10\n\n\n\n\n\nBased on this analysis by Jeremy Howard, it might make sense for us to try a different model to improve our error rate. Let’s try the convnext models.\ntimm.list_models('convnext*')\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.507469\n\n\n0.354891\n\n\n0.090452\n\n\n00:09\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.174055\n\n\n0.094325\n\n\n0.040201\n\n\n00:08\n\n\n\n\n1\n\n\n0.131543\n\n\n0.100523\n\n\n0.045226\n\n\n00:10\n\n\n\n\n2\n\n\n0.093354\n\n\n0.084719\n\n\n0.045226\n\n\n00:09\n\n\n\n\n\nIndeed, using the convnext models, our error rate has dropped from 0.105528 to 0.045226! Hot dog!\nLet’s export the trained model so that it can be saved and later loaded for further training without needing to retrain the model from scratch.\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#deployment",
    "href": "posts/hot-dog-classifier/index.html#deployment",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Deployment",
    "text": "Deployment\nHaving created our model, we now need to showcase our project to the world at large! Hugging Face Spaces (HFS) is a platform on which we can do so. We will make use of HFS, in addition to Gradio, an open-source library that enables one to create a simple interface for a machine learning model. To see how to pair HFS with Gradio, I encourage you to check out this concise blog post by Tanishq Abraham.\nBefore deploying out project, we will need to make an app.py file. This file will make use of Gradio to create an interface to classify images using our pre-trained machine learning model (in this case, our model.pkl file).\nHere’s my code for the app.py file:\n# AUTOGENERATED! DO NOT EDIT! File to edit: . (unless otherwise specified).\n\n__all__ = ['learn', 'classify_image', 'categories', 'image', 'label', 'examples', 'intf']\n\n# Cell\nfrom fastai.vision.all import *\nimport gradio as gr\n\n# Cell\nlearn = load_learner('model.pkl')\n\n# Cell\ncategories = learn.dls.vocab\n\ndef classify_image(img):\n    pred,idx,probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\n# Cell\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\nexamples = ['hot_dog.jpeg']\n\n# Cell\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch()\nThis code creates a simple interactive interface where users can upload images, click a submit button, and get predictions from the model. For more information regarding the project’s files, please see this link.\nLet’s now play around with the deployed project! Let’s grab a random image of both a hot dog and a “not hot dog” (in this case, a taco).\n\n\n \n\nTesting our model on both pictures, we get the following results:\n\n \n\nOur model seems to perform exceptionally well!\nHowever, it is important to consider that there are still some edge cases in which the model performs rather poorly; for instance, when the structure of a food item is extremely similar to that of a hot dog…\n\n\n\n\nSub sandwich\n\n\n\nTo improve this model, we should thus try including more images of “subs” in the not-hot-dog subdirectory."
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#acknowledgments",
    "href": "posts/hot-dog-classifier/index.html#acknowledgments",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank the teams at DeepLearning.AI and fast.ai, from both of which I have been able to learn a lot about deep learning in the preceding months."
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#disclaimer",
    "href": "posts/hot-dog-classifier/index.html#disclaimer",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Disclaimer",
    "text": "Disclaimer\nSome readers may wonder if a certain male appendage is able to fool this classifier. I leave all such curiosities to the explorations of the reader…"
  }
]