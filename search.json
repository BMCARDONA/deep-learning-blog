[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Recent college graduate, and a deep learning practitioner!\nYou can find my personal website here."
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html",
    "href": "posts/hot-dog-classifier/index.html",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "",
    "text": "Having recently completed DeepLearning.AI’s wonderful Deep Learning Specialization, and having recently started fast.ai’s Practical Deep Learning for Coders, I thought I would try to implement a binary classifier to test whether an image of a food item belongs to the “hot dog” or “not hot dog” class, as seen on that Silicon Valley episode.\nTo create this project, I used the fast.ai library, Gradio, HuggingFace Spaces, this Kaggle dataset, and Google Colab. In this article, we will discuss the notebook I used to train my model, in addition to the steps I took to deploy it. Feel free to check out the deployed project here. :)"
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#training",
    "href": "posts/hot-dog-classifier/index.html#training",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Training",
    "text": "Training\nTo begin, we will import any necessary dependencies.\nfrom fastai.vision.all import *\nimport timm\nfrom google.colab import drive\nimport os\nSince we are using Google Colab to execute the notebook cells, we need to mount the Google Drive to the Colab notebook’s file system. (Mounting allows one to access and manipulate files stored in one’s Google Drive directly from within one’s Colab notebook.)\n# Mount Google Drive\ndrive.mount('/content/drive')\nHaving mounted my drive, let’s now specify the path to my dataset directory, which itself contains two additional subdirectories: hot-dog and not-hot-dog. The former contains photos of hot dogs, the latter photos of “not hot dogs.”\npath = '/content/drive/MyDrive/fast_ai_experiments/3_neural_net_foundations/hot_dog_not_hotdog/dataset/'\nEvery image in the hot-dog and not-hot-dog subdirectories has a pre-existing naming format of “number.jpg” (e.g., “1231.jpg”). For the sake of using a better naming format, let’s use the format of “hot-dog_index” (e.g., “hot-dog_12.jpg”) for each image in the hot-dog subdirectory, and “not-hot-dog_index” (e.g., “not-hot-dog_12.jpg”) for each image in the not-hot-dog subdirectory.\n# List of subdirectories\nsubdirectories = ['hot-dog', 'not-hot-dog']\n\n# Iterate through subdirectories\nfor subdir in subdirectories:\n    subdir_path = os.path.join(path, subdir)\n\n    # List all files in the subdirectory\n    file_list = os.listdir(subdir_path)\n\n    # Iterate through the files and rename them with a numbered sequence\n    for i, filename in enumerate(file_list, start=1):\n        if filename.endswith(\".jpg\"):\n            new_filename = f\"{subdir}_{i}.jpg\"\n            os.rename(os.path.join(subdir_path, filename), os.path.join(subdir_path, new_filename))\nNext, we will use the ImageDataLoaders.from_name_func() method. This is a fast.ai method used for creating “data loaders” for image classification tasks; it takes various arguments, which define how the data should be loaded and prepared.\nUsing this method, we will define the training/validation split as 80% for training and 20% for validation; we will label each image in the hot-dog subdirectory as “hot-dog” and each image in the not-hot-dog one as “not-hot-dog”; and we will re-size each image to be 224 x 224 in pixel size.\n# Creating ImageDataLoaders\ndls = ImageDataLoaders.from_name_func(\n    path,\n    get_image_files(path),\n    valid_pct=0.2,\n    seed=42,\n    label_func=RegexLabeller(pat = r'^([^/]+)_\\d+'),\n    item_tfms=Resize(224),\n)\nLet’s now take a look at a batch containing 20 labeled images:\ndls.show_batch(max_n=20)\n\n\n\npng\n\n\nNice, it seems that each photo is labeled appropriately! Let’s now use the fast.ai library to harness the capabilities of transfer learning. We will create a learner object for image classification using the ResNet-34 architecture, train the model on our training set for 3 epochs, and then evaluate the model’s performance on our validation set using the “error rate” metric.\nlearn = vision_learner(dls, resnet34, metrics=error_rate)\nlearn.fine_tune(3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.890783\n\n\n0.328621\n\n\n0.130653\n\n\n02:10\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.457683\n\n\n0.231882\n\n\n0.105528\n\n\n00:13\n\n\n\n\n1\n\n\n0.270772\n\n\n0.355318\n\n\n0.110553\n\n\n00:08\n\n\n\n\n2\n\n\n0.187048\n\n\n0.347728\n\n\n0.105528\n\n\n00:10\n\n\n\n\n\nBased on this analysis by Jeremy Howard, it might make sense for us to try a different model to improve our error rate. Let’s try the convnext models.\ntimm.list_models('convnext*')\n['convnext_atto',\n 'convnext_atto_ols',\n 'convnext_base',\n 'convnext_femto',\n 'convnext_femto_ols',\n 'convnext_large',\n 'convnext_large_mlp',\n 'convnext_nano',\n 'convnext_nano_ols',\n 'convnext_pico',\n 'convnext_pico_ols',\n 'convnext_small',\n 'convnext_tiny',\n 'convnext_tiny_hnf',\n 'convnext_xlarge',\n 'convnext_xxlarge',\n 'convnextv2_atto',\n 'convnextv2_base',\n 'convnextv2_femto',\n 'convnextv2_huge',\n 'convnextv2_large',\n 'convnextv2_nano',\n 'convnextv2_pico',\n 'convnextv2_small',\n 'convnextv2_tiny']\nlearn = vision_learner(dls, 'convnext_tiny_in22k', metrics=error_rate).to_fp16()\nlearn.fine_tune(3)\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.507469\n\n\n0.354891\n\n\n0.090452\n\n\n00:09\n\n\n\n\n\n\n\n\n\n\n\nepoch\n\n\ntrain_loss\n\n\nvalid_loss\n\n\nerror_rate\n\n\ntime\n\n\n\n\n\n\n0\n\n\n0.174055\n\n\n0.094325\n\n\n0.040201\n\n\n00:08\n\n\n\n\n1\n\n\n0.131543\n\n\n0.100523\n\n\n0.045226\n\n\n00:10\n\n\n\n\n2\n\n\n0.093354\n\n\n0.084719\n\n\n0.045226\n\n\n00:09\n\n\n\n\n\nIndeed, using the convnext models, our error rate has dropped from 0.105528 to 0.045226! Hot dog!\nLet’s export the trained model so that it can be saved and later loaded for further training without needing to retrain the model from scratch.\nlearn.export('model.pkl')"
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#deployment",
    "href": "posts/hot-dog-classifier/index.html#deployment",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Deployment",
    "text": "Deployment\nHaving created our model, we now need to showcase our project to the world at large! Hugging Face Spaces (HFS) is a platform on which we can do so. We will make use of HFS, in addition to Gradio, an open-source library that enables one to create a simple interface for a machine learning model. To see how to pair HFS with Gradio, I encourage you to check out this concise blog post by Tanishq Abraham.\nBefore deploying out project, we will need to make an app.py file. This file will make use of Gradio to create an interface to classify images using our pre-trained machine learning model (in this case, our model.pkl file).\nHere’s my code for the app.py file:\n# AUTOGENERATED! DO NOT EDIT! File to edit: . (unless otherwise specified).\n\n__all__ = ['learn', 'classify_image', 'categories', 'image', 'label', 'examples', 'intf']\n\n# Cell\nfrom fastai.vision.all import *\nimport gradio as gr\n\n# Cell\nlearn = load_learner('model.pkl')\n\n# Cell\ncategories = learn.dls.vocab\n\ndef classify_image(img):\n    pred,idx,probs = learn.predict(img)\n    return dict(zip(categories, map(float,probs)))\n\n# Cell\nimage = gr.inputs.Image(shape=(192, 192))\nlabel = gr.outputs.Label()\nexamples = ['hot_dog.jpeg']\n\n# Cell\nintf = gr.Interface(fn=classify_image, inputs=image, outputs=label, examples=examples)\nintf.launch()\nThis code creates a simple interactive interface where users can upload images, click a submit button, and get predictions from the model. For more information regarding the project’s files, please see this link.\nLet’s now play around with the deployed project! Let’s grab a random image of both a hot dog and a “not hot dog” (in this case, a taco).\n\n\n \n\nTesting our model on both pictures, we get the following results:\n\n \n\nOur model seems to perform exceptionally well!\nHowever, it is important to consider that there are still some edge cases in which the model performs rather poorly; for instance, when the structure of a food item is extremely similar to that of a hot dog…\n\n\n\n\nSub sandwich\n\n\n\nTo improve this model, we should thus try including more images of “subs” in the not-hot-dog subdirectory."
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#acknowledgments",
    "href": "posts/hot-dog-classifier/index.html#acknowledgments",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank the teams at DeepLearning.AI and fast.ai, from both of which I have been able to learn a lot about deep learning in the preceding months."
  },
  {
    "objectID": "posts/hot-dog-classifier/index.html#disclaimer",
    "href": "posts/hot-dog-classifier/index.html#disclaimer",
    "title": "Creating a Hot Dog Binary Classifer Using Fast.AI",
    "section": "Disclaimer",
    "text": "Disclaimer\nSome readers may wonder if a certain male appendage is able to fool this classifier. I leave all such curiosities to the explorations of the reader…"
  },
  {
    "objectID": "posts/good-resources/index.html",
    "href": "posts/good-resources/index.html",
    "title": "A List of Great Books and Articles",
    "section": "",
    "text": "I recently came across some great books and articles which cover deep-learning-related subjects. To prevent myself from losing track of these gems, I thought I would list them here so that I can come back to them in the future if need be. Happy learning!\n\nBooks\n\nIntroduction to Statistical Learning by Gareth James, Daniela Witten, Trevor Hastie, and Rob Robshirani\n\n\n\nArticles\n\nThe Unreasonable Effectiveness of Recurrent Neural Networks by Andrej Karpathy\nUnderstanding LSTM Networks by Christopher Olah"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bradley’s Deep Learning Blog",
    "section": "",
    "text": "A List of Great Books and Articles\n\n\n\n\n\n\n\nbooks\n\n\narticles\n\n\n\n\n\n\n\n\n\n\n\nSep 4, 2023\n\n\nBradley Cardona\n\n\n\n\n\n\n  \n\n\n\n\nCreating a CNN using the PyTorch Framework\n\n\n\n\n\n\n\ncode\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nSep 1, 2023\n\n\nBradley Cardona\n\n\n\n\n\n\n  \n\n\n\n\nCreating a Hot Dog Binary Classifer Using Fast.AI\n\n\n\n\n\n\n\ncode\n\n\nproject\n\n\n\n\n\n\n\n\n\n\n\nAug 17, 2023\n\n\nBradley Cardona\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to the Blog!\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 14, 2023\n\n\nBradley Cardona\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/pytorch-model/index.html",
    "href": "posts/pytorch-model/index.html",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "I recently embarked on a project to implement a model using PyTorch. This led me to delve deep into the PyTorch framework, and to make use of transfer learning (as I did in my last blog post). In this post, I share both what I learned from this experience, and how you can create a similar PyTorch model of you own. This post is inspired greatly by a post by Sasank Chilamkurthy, whose excellent tutorial you can find here. This post also makes use of Rohit Rawat’s Crocodylia dataset.\n\n\nIn the field of deep learning, few people start by training an entire convolutional neural network (ConvNet) from scratch, primarily due to the scarcity of large datasets. Instead, a common practice is to employ transfer learning. This approach entails utilizing a model that has been pre-trained on an extensive dataset like ImageNet, which contains 1.2 million images across 1000 categories. To do this, we freeze the weights of the pre-trained neural network layers except a few of the last fully connected layers. We can then replace the last layer with a new one (to making the model capable of being used as, for example, a binary classifier), and then train this new layer to make the model’s predictions more accurate.\nLet’s see how all of this can be done using the PyTorch framework!\n\n\n\nTo kickstart our exploration, we need a dataset. In this tutorial, we’re working with a small dataset containing images of alligators and crocodiles, with ~350 training images per class and ~70 validation images per class. You can download the dataset here.\nWe’ll leverage PyTorch’s torchvision and torch.utils.data packages for data loading. To get started, we define data transformations for both the training and validation datasets.\nimport os\nimport torch\nfrom torchvision import datasets, transforms\n\n# Define data transformations for both training and validation datasets\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\nData transformations are defined using PyTorch’s transforms module. These transformations are applied to both the training and validation datasets to preprocess the images. The transformations include resizing, random cropping, horizontal flipping, converting to tensors, and normalization.\nNext, we mount Google Drive to access the dataset. (I used Google Colab to execute my code cells. This step may be slightly different for you depending on the platform you use to execute your own code cells.) You should update data_dir with your dataset location.\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndata_dir = '/content/drive/MyDrive/hymenoptera_data'  # Update this path to your dataset location\nWith the data transformations in place, we can create datasets and data loaders for both training and validation sets.\n# Create datasets for both training and validation\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n                  for x in ['train', 'val']}\n\n# Create data loaders for both training and validation datasets\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\n\n# Store the sizes of the training and validation datasets\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n# Get the class names (e.g., 'alligators' and 'crocodiles') from the training dataset\nclass_names = image_datasets['train'].classes\n\n# Check if a CUDA-enabled GPU is available and set the device accordingly\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nDatasets and data loaders are created for both the training and validation sets. We use the ImageFolder class to load images from the specified directories and apply the previously defined data transformations. Data loaders are used to load batches of data during training and validation. The device is also determined based on the availability of a GPU.\n\n\n\nLet’s start by visualizing a few training images to get a better understanding of the applied data augmentation techniques.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision\n\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from the batch of images\nout = torchvision.utils.make_grid(inputs)\n\n# Display the grid of images with their corresponding class names as titles\nimshow(out, title=[class_names[x] for x in classes])\nHere, we define a function (imshow) to display images and then uses it to visualize a batch of training data. Specifically, this function shows a grid of images with their corresponding class names as titles.\n\n\n\nNice, our function appears to be working well.\n\n\n\nNow, let’s write a general function to train a model. This function encompasses several key aspects of deep learning training, including learning rate scheduling and model checkpointing.\nimport time\nimport os\nimport torch\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n    \n    with TemporaryDirectory() as tempdir:\n        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n        \n        torch.save(model.state_dict(), best_model_params_path)\n        best_acc = 0.0\n\n        for epoch in range(num_epochs):\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                running_loss = 0.0\n                running_corrects = 0\n\n                for inputs, labels in dataloaders[phase]:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n\n                if phase == 'train':\n                    scheduler.step()\n\n                epoch_loss = running_loss / dataset_sizes[phase]\n                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n                if phase == 'val' and epoch_acc &gt; best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict\n\n(), best_model_params_path)\n\n            print()\n\n        time_elapsed = time.time() - since\n        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print(f'Best val Acc: {best_acc:.4f}')\n\n        model.load_state_dict(torch.load(best_model_params_path))\n        return model\nWe define a function (train_model) to train a deep learning model. It includes training and validation loops, tracks loss and accuracy, and saves the best model parameters based on validation accuracy. Learning rate scheduling is also applied.\n\n\n\nLet’s create a function to visualize model predictions. This will help us understand how well the model is performing on specific images.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images // 2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'Predicted: {class_names[preds[j]]}')\n\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n\n        model.train(mode=was_training)\nWe define another function (visualize_model) to visualize model predictions. It takes a trained model, makes predictions on a batch of validation data, and displays the input images along with their respective predicted class labels.\n\n\n\nNow comes the exciting part—finetuning a pretrained model for our specific task. In this example, we’ll use the popular ResNet18 architecture pretrained on ImageNet and adapt it for our binary classification task (alligators vs. crocodiles).\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load the pretrained ResNet-18 model\nmodel_ft = models.resnet18(pretrained=True)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay the learning rate by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n# Move the model to the GPU if available\nmodel_ft = model_ft.to(device)\nIn this code block, a pretrained ResNet-18 model is loaded and modified for binary classification. The final fully connected layer is replaced with a new one with 2 output units (for alligators and crocodiles classification). The loss function, optimizer, and learning rate scheduler are defined. The model is moved to the GPU if available.\nNow that everything is set up, we can proceed to train the model using the train_model function defined earlier.\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\nWe train the fine-tuned model using the train_model function defined earlier. It prints the training and validation loss and accuracy for each epoch and records the best model parameters based on validation accuracy.\nEpoch 0/24\n----------\ntrain Loss: 0.7276 Acc: 0.6117\nval Loss: 0.4810 Acc: 0.7767\n\nEpoch 1/24\n----------\ntrain Loss: 0.7097 Acc: 0.6749\nval Loss: 0.5373 Acc: 0.7573\n\nEpoch 2/24\n----------\ntrain Loss: 0.9122 Acc: 0.6185\nval Loss: 0.5871 Acc: 0.7379\n\n…\n\nEpoch 23/24\n----------\ntrain Loss: 0.3144 Acc: 0.8555\nval Loss: 0.4539 Acc: 0.7864\n\nEpoch 24/24\n----------\ntrain Loss: 0.3832 Acc: 0.8330\nval Loss: 0.4158 Acc: 0.7961\n\nTraining complete in 9m 19s\nBest val Acc: 0.8350\nAfter training, our best validation accuracy is recorded as 0.8350. But can we do better?\n\n\n\nTo see how our finetuned model performs, we can use the visualize_model function.\nvisualize_model(model_ft)\nThis code block visualizes the predictions of the fine-tuned model on a batch of validation data, allowing us to see how well the model is performing on specific images.\n\n   \n\n\n\n\nIn the previous example, we finetuned the entire model. However, sometimes we might want to use a pretrained ConvNet as a fixed feature extractor. In this case, we’ll freeze all layers except the final fully connected layer. This can be useful when you have a small dataset and you don’t want to risk overfitting by finetuning the entire model.\nLet’s create a function to set the requires_grad attribute of the model’s layers for feature extraction.\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\nWe define a function (set_parameter_requires_grad) to freeze the weights of layers in a model. It sets requires_grad to False for all parameters if feature_extracting is True.\nNext, we’ll (again) load a pretrained ResNet18 model and replace the final fully connected layer for our binary classification task; however, this time will freeze all layers except the final fully connected layer.\n# Load the pretrained ResNet-18 model\nmodel_conv = models.resnet18(pretrained=True)\n\n# Freeze all layers except the final fully connected layer\nset_parameter_requires_grad(model_conv, feature_extracting=True)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only the parameters of the final layer are being optimized\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay the learning rate by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n# Move the model to the GPU if available\nmodel_conv = model_conv.to(device)\nThis freezing technique is frequently used when you want to use a pretrained model as a fixed feature extractor. The rest of the code is similar to the fine-tuning scenario, defining loss, optimizer, and learning rate scheduler and moving the model to the GPU.\nNow, we can train the model using the same train_model function as before.\nmodel_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)\nAgain, we print the training and validation loss and accuracy for each epoch and record the best model parameters based on validation accuracy.\nEpoch 0/24\n----------\ntrain Loss: 0.7446 Acc: 0.5350\nval Loss: 0.4117 Acc: 0.7864\n\nEpoch 1/24\n----------\ntrain Loss: 0.6309 Acc: 0.6682\nval Loss: 0.3449 Acc: 0.8447\n\nEpoch 2/24\n----------\ntrain Loss: 0.6819 Acc: 0.6614\nval Loss: 0.6134 Acc: 0.6990\n\n…\n\nEpoch 23/24\n----------\ntrain Loss: 0.5224 Acc: 0.7381\nval Loss: 0.3565 Acc: 0.8738\n\nEpoch 24/24\n----------\ntrain Loss: 0.5292 Acc: 0.7336\nval Loss: 0.3602 Acc: 0.8738\n\nTraining complete in 8m 42s\nBest val Acc: 0.8835\nAfter freezing the layers, our best validation accuracy is now recorded as 0.8835 – great!\nLet’s now visualize our model!\nvisualize_model(model_conv)\n\nplt.ioff()\nplt.show()\n\n   \n\n\n\n\nWhile it is interesting to see how well our model performs on a pre-made dataset, it will doubtless be more exciting if we can test our model on any image pulled from Google. Allegheny College (of which I am now an alumnus!) has a statue of an alligator on its campus. For fun, then, let’s see how well our model performs when fed an image of this very statue. Here’s the image:\n\n\n\nLet’s create a function that takes a model and an image as input, processes the image, makes predictions using the model, and displays the image along with its respective predicted class label.\ndef visualize_model_predictions(model, img_path):\n    # Store the current training state of the model\n    was_training = model.training\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Open and preprocess the image from the specified path\n    img = Image.open(img_path)\n    img = data_transforms['val'](img)  # Apply the same transformations as for validation data\n    img = img.unsqueeze(0)  # Add a batch dimension\n    img = img.to(device)  # Move the image to the specified device (GPU or CPU)\n\n    # Disable gradient computation during inference\n    with torch.no_grad():\n        # Forward pass to obtain predictions\n        outputs = model(img)\n        _, preds = torch.max(outputs, 1)\n\n        # Display the image and its predicted class\n        ax = plt.subplot(2, 2, 1)  # Create a subplot\n        ax.axis('off')  # Turn off axis labels\n        ax.set_title(f'Predicted: {class_names[preds[0]]}')  # Set the title with the predicted class name\n        imshow(img.cpu().data[0])  # Display the image\n\n        # Restore the model's original training state\n        model.train(mode=was_training)\nWe can now predict the label of our image.\n# Visualize predictions for a single image using the specified model\nvisualize_model_predictions(\n    model_conv,\n    img_path='/content/drive/MyDrive/crocodylia_data/alligator.jpeg'  # Specify the path to the image\n)\n\n# Turn off interactive mode for matplotlib to display the plot\nplt.ioff()\n\n# Display the plot with the image and its predictions\nplt.show()\n\n\n\nIndeed, the statue is in fact one of an alligator! Wonderful!\n\n\n\nUsing transfer learning with PyTorch, we’ve explored how we can finetune a pretrained ConvNet and employ it to a novel sitation (in this case, a small dataset of alligator and crocodile images). By harnessing the knowledge encoded in pretrained models, we can accelerate our model development process and achieve great results with relatively small datasets!"
  },
  {
    "objectID": "posts/pytorch-model/index.html#transfer-learning-a-brief-overview",
    "href": "posts/pytorch-model/index.html#transfer-learning-a-brief-overview",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "In the field of deep learning, few people start by training an entire convolutional neural network (ConvNet) from scratch, primarily due to the scarcity of large datasets. Instead, a common practice is to employ transfer learning. This approach entails utilizing a model that has been pre-trained on an extensive dataset like ImageNet, which contains 1.2 million images across 1000 categories. To do this, we freeze the weights of the pre-trained neural network layers except a few of the last fully connected layers. We can then replace the last layer with a new one (to making the model capable of being used as, for example, a binary classifier), and then train this new layer to make the model’s predictions more accurate.\nLet’s see how all of this can be done using the PyTorch framework!"
  },
  {
    "objectID": "posts/pytorch-model/index.html#loading-the-data",
    "href": "posts/pytorch-model/index.html#loading-the-data",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "To kickstart our exploration, we need a dataset. In this tutorial, we’re working with a small dataset containing images of alligators and crocodiles, with ~350 training images per class and ~70 validation images per class. You can download the dataset here.\nWe’ll leverage PyTorch’s torchvision and torch.utils.data packages for data loading. To get started, we define data transformations for both the training and validation datasets.\nimport os\nimport torch\nfrom torchvision import datasets, transforms\n\n# Define data transformations for both training and validation datasets\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\nData transformations are defined using PyTorch’s transforms module. These transformations are applied to both the training and validation datasets to preprocess the images. The transformations include resizing, random cropping, horizontal flipping, converting to tensors, and normalization.\nNext, we mount Google Drive to access the dataset. (I used Google Colab to execute my code cells. This step may be slightly different for you depending on the platform you use to execute your own code cells.) You should update data_dir with your dataset location.\nfrom google.colab import drive\ndrive.mount('/content/drive')\ndata_dir = '/content/drive/MyDrive/hymenoptera_data'  # Update this path to your dataset location\nWith the data transformations in place, we can create datasets and data loaders for both training and validation sets.\n# Create datasets for both training and validation\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n                  for x in ['train', 'val']}\n\n# Create data loaders for both training and validation datasets\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\n\n# Store the sizes of the training and validation datasets\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n\n# Get the class names (e.g., 'alligators' and 'crocodiles') from the training dataset\nclass_names = image_datasets['train'].classes\n\n# Check if a CUDA-enabled GPU is available and set the device accordingly\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nDatasets and data loaders are created for both the training and validation sets. We use the ImageFolder class to load images from the specified directories and apply the previously defined data transformations. Data loaders are used to load batches of data during training and validation. The device is also determined based on the availability of a GPU."
  },
  {
    "objectID": "posts/pytorch-model/index.html#visualizing-data",
    "href": "posts/pytorch-model/index.html#visualizing-data",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "Let’s start by visualizing a few training images to get a better understanding of the applied data augmentation techniques.\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torchvision\n\ndef imshow(inp, title=None):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from the batch of images\nout = torchvision.utils.make_grid(inputs)\n\n# Display the grid of images with their corresponding class names as titles\nimshow(out, title=[class_names[x] for x in classes])\nHere, we define a function (imshow) to display images and then uses it to visualize a batch of training data. Specifically, this function shows a grid of images with their corresponding class names as titles.\n\n\n\nNice, our function appears to be working well."
  },
  {
    "objectID": "posts/pytorch-model/index.html#training-the-model",
    "href": "posts/pytorch-model/index.html#training-the-model",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "Now, let’s write a general function to train a model. This function encompasses several key aspects of deep learning training, including learning rate scheduling and model checkpointing.\nimport time\nimport os\nimport torch\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n    \n    with TemporaryDirectory() as tempdir:\n        best_model_params_path = os.path.join(tempdir, 'best_model_params.pt')\n        \n        torch.save(model.state_dict(), best_model_params_path)\n        best_acc = 0.0\n\n        for epoch in range(num_epochs):\n            print(f'Epoch {epoch}/{num_epochs - 1}')\n            print('-' * 10)\n\n            for phase in ['train', 'val']:\n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n\n                running_loss = 0.0\n                running_corrects = 0\n\n                for inputs, labels in dataloaders[phase]:\n                    inputs = inputs.to(device)\n                    labels = labels.to(device)\n\n                    optimizer.zero_grad()\n\n                    with torch.set_grad_enabled(phase == 'train'):\n                        outputs = model(inputs)\n                        _, preds = torch.max(outputs, 1)\n                        loss = criterion(outputs, labels)\n\n                        if phase == 'train':\n                            loss.backward()\n                            optimizer.step()\n\n                    running_loss += loss.item() * inputs.size(0)\n                    running_corrects += torch.sum(preds == labels.data)\n\n                if phase == 'train':\n                    scheduler.step()\n\n                epoch_loss = running_loss / dataset_sizes[phase]\n                epoch_acc = running_corrects.double() / dataset_sizes[phase]\n\n                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n\n                if phase == 'val' and epoch_acc &gt; best_acc:\n                    best_acc = epoch_acc\n                    torch.save(model.state_dict\n\n(), best_model_params_path)\n\n            print()\n\n        time_elapsed = time.time() - since\n        print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n        print(f'Best val Acc: {best_acc:.4f}')\n\n        model.load_state_dict(torch.load(best_model_params_path))\n        return model\nWe define a function (train_model) to train a deep learning model. It includes training and validation loops, tracks loss and accuracy, and saves the best model parameters based on validation accuracy. Learning rate scheduling is also applied."
  },
  {
    "objectID": "posts/pytorch-model/index.html#visualizing-model-predictions",
    "href": "posts/pytorch-model/index.html#visualizing-model-predictions",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "Let’s create a function to visualize model predictions. This will help us understand how well the model is performing on specific images.\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images // 2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title(f'Predicted: {class_names[preds[j]]}')\n\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n\n        model.train(mode=was_training)\nWe define another function (visualize_model) to visualize model predictions. It takes a trained model, makes predictions on a batch of validation data, and displays the input images along with their respective predicted class labels."
  },
  {
    "objectID": "posts/pytorch-model/index.html#finetuning-the-model",
    "href": "posts/pytorch-model/index.html#finetuning-the-model",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "Now comes the exciting part—finetuning a pretrained model for our specific task. In this example, we’ll use the popular ResNet18 architecture pretrained on ImageNet and adapt it for our binary classification task (alligators vs. crocodiles).\nfrom torchvision import models\nimport torch.nn as nn\nimport torch.optim as optim\n\n# Load the pretrained ResNet-18 model\nmodel_ft = models.resnet18(pretrained=True)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay the learning rate by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n# Move the model to the GPU if available\nmodel_ft = model_ft.to(device)\nIn this code block, a pretrained ResNet-18 model is loaded and modified for binary classification. The final fully connected layer is replaced with a new one with 2 output units (for alligators and crocodiles classification). The loss function, optimizer, and learning rate scheduler are defined. The model is moved to the GPU if available.\nNow that everything is set up, we can proceed to train the model using the train_model function defined earlier.\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler, num_epochs=25)\nWe train the fine-tuned model using the train_model function defined earlier. It prints the training and validation loss and accuracy for each epoch and records the best model parameters based on validation accuracy.\nEpoch 0/24\n----------\ntrain Loss: 0.7276 Acc: 0.6117\nval Loss: 0.4810 Acc: 0.7767\n\nEpoch 1/24\n----------\ntrain Loss: 0.7097 Acc: 0.6749\nval Loss: 0.5373 Acc: 0.7573\n\nEpoch 2/24\n----------\ntrain Loss: 0.9122 Acc: 0.6185\nval Loss: 0.5871 Acc: 0.7379\n\n…\n\nEpoch 23/24\n----------\ntrain Loss: 0.3144 Acc: 0.8555\nval Loss: 0.4539 Acc: 0.7864\n\nEpoch 24/24\n----------\ntrain Loss: 0.3832 Acc: 0.8330\nval Loss: 0.4158 Acc: 0.7961\n\nTraining complete in 9m 19s\nBest val Acc: 0.8350\nAfter training, our best validation accuracy is recorded as 0.8350. But can we do better?"
  },
  {
    "objectID": "posts/pytorch-model/index.html#visualizing-model-predictions-1",
    "href": "posts/pytorch-model/index.html#visualizing-model-predictions-1",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "To see how our finetuned model performs, we can use the visualize_model function.\nvisualize_model(model_ft)\nThis code block visualizes the predictions of the fine-tuned model on a batch of validation data, allowing us to see how well the model is performing on specific images."
  },
  {
    "objectID": "posts/pytorch-model/index.html#convnet-as-a-fixed-feature-extractor",
    "href": "posts/pytorch-model/index.html#convnet-as-a-fixed-feature-extractor",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "In the previous example, we finetuned the entire model. However, sometimes we might want to use a pretrained ConvNet as a fixed feature extractor. In this case, we’ll freeze all layers except the final fully connected layer. This can be useful when you have a small dataset and you don’t want to risk overfitting by finetuning the entire model.\nLet’s create a function to set the requires_grad attribute of the model’s layers for feature extraction.\ndef set_parameter_requires_grad(model, feature_extracting):\n    if feature_extracting:\n        for param in model.parameters():\n            param.requires_grad = False\nWe define a function (set_parameter_requires_grad) to freeze the weights of layers in a model. It sets requires_grad to False for all parameters if feature_extracting is True.\nNext, we’ll (again) load a pretrained ResNet18 model and replace the final fully connected layer for our binary classification task; however, this time will freeze all layers except the final fully connected layer.\n# Load the pretrained ResNet-18 model\nmodel_conv = models.resnet18(pretrained=True)\n\n# Freeze all layers except the final fully connected layer\nset_parameter_requires_grad(model_conv, feature_extracting=True)\n\n# Modify the final fully connected layer for binary classification\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 2)\n\n# Define loss function and optimizer\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only the parameters of the final layer are being optimized\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay the learning rate by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n# Move the model to the GPU if available\nmodel_conv = model_conv.to(device)\nThis freezing technique is frequently used when you want to use a pretrained model as a fixed feature extractor. The rest of the code is similar to the fine-tuning scenario, defining loss, optimizer, and learning rate scheduler and moving the model to the GPU.\nNow, we can train the model using the same train_model function as before.\nmodel_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)\nAgain, we print the training and validation loss and accuracy for each epoch and record the best model parameters based on validation accuracy.\nEpoch 0/24\n----------\ntrain Loss: 0.7446 Acc: 0.5350\nval Loss: 0.4117 Acc: 0.7864\n\nEpoch 1/24\n----------\ntrain Loss: 0.6309 Acc: 0.6682\nval Loss: 0.3449 Acc: 0.8447\n\nEpoch 2/24\n----------\ntrain Loss: 0.6819 Acc: 0.6614\nval Loss: 0.6134 Acc: 0.6990\n\n…\n\nEpoch 23/24\n----------\ntrain Loss: 0.5224 Acc: 0.7381\nval Loss: 0.3565 Acc: 0.8738\n\nEpoch 24/24\n----------\ntrain Loss: 0.5292 Acc: 0.7336\nval Loss: 0.3602 Acc: 0.8738\n\nTraining complete in 8m 42s\nBest val Acc: 0.8835\nAfter freezing the layers, our best validation accuracy is now recorded as 0.8835 – great!\nLet’s now visualize our model!\nvisualize_model(model_conv)\n\nplt.ioff()\nplt.show()"
  },
  {
    "objectID": "posts/pytorch-model/index.html#inference-on-custom-images",
    "href": "posts/pytorch-model/index.html#inference-on-custom-images",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "While it is interesting to see how well our model performs on a pre-made dataset, it will doubtless be more exciting if we can test our model on any image pulled from Google. Allegheny College (of which I am now an alumnus!) has a statue of an alligator on its campus. For fun, then, let’s see how well our model performs when fed an image of this very statue. Here’s the image:\n\n\n\nLet’s create a function that takes a model and an image as input, processes the image, makes predictions using the model, and displays the image along with its respective predicted class label.\ndef visualize_model_predictions(model, img_path):\n    # Store the current training state of the model\n    was_training = model.training\n    # Set the model to evaluation mode\n    model.eval()\n\n    # Open and preprocess the image from the specified path\n    img = Image.open(img_path)\n    img = data_transforms['val'](img)  # Apply the same transformations as for validation data\n    img = img.unsqueeze(0)  # Add a batch dimension\n    img = img.to(device)  # Move the image to the specified device (GPU or CPU)\n\n    # Disable gradient computation during inference\n    with torch.no_grad():\n        # Forward pass to obtain predictions\n        outputs = model(img)\n        _, preds = torch.max(outputs, 1)\n\n        # Display the image and its predicted class\n        ax = plt.subplot(2, 2, 1)  # Create a subplot\n        ax.axis('off')  # Turn off axis labels\n        ax.set_title(f'Predicted: {class_names[preds[0]]}')  # Set the title with the predicted class name\n        imshow(img.cpu().data[0])  # Display the image\n\n        # Restore the model's original training state\n        model.train(mode=was_training)\nWe can now predict the label of our image.\n# Visualize predictions for a single image using the specified model\nvisualize_model_predictions(\n    model_conv,\n    img_path='/content/drive/MyDrive/crocodylia_data/alligator.jpeg'  # Specify the path to the image\n)\n\n# Turn off interactive mode for matplotlib to display the plot\nplt.ioff()\n\n# Display the plot with the image and its predictions\nplt.show()\n\n\n\nIndeed, the statue is in fact one of an alligator! Wonderful!"
  },
  {
    "objectID": "posts/pytorch-model/index.html#conclusion",
    "href": "posts/pytorch-model/index.html#conclusion",
    "title": "Creating a CNN using the PyTorch Framework",
    "section": "",
    "text": "Using transfer learning with PyTorch, we’ve explored how we can finetune a pretrained ConvNet and employ it to a novel sitation (in this case, a small dataset of alligator and crocodile images). By harnessing the knowledge encoded in pretrained models, we can accelerate our model development process and achieve great results with relatively small datasets!"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome to the Blog!",
    "section": "",
    "text": "The first post in my deep learning blog. Welcome!\nMy name’s Bradley, and I’m a recent college graduate and deep learning practitioner. For the last few months, I have been spending much of my time learning about deep learning, in addition to machine learning and artificial intelligence, more broadly. My aim for this blog is threefold: to explore the technical developments of deep learning – from Convolutional Neural Networks (CNNs) and Recurrent Neural Networds (RNNS), to Long Short Term Memory Networks (LSTMs) and Large Language Models (LLMS) – to talk through my own personal projects, and, most importantly, to explore the ethical implications of the field as a whole, on the world at large. I am brimful of energy to be entering into this conversation; I have had many thoughts about deep learning for a while now, and am eager to express my own viewpoints on the subject."
  }
]